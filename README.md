# Pipeline ETL con AWS Glue y S3 - Gu√≠a Visual

## üìã Descripci√≥n

Este proyecto implementa un pipeline ETL (Extract, Transform, Load) usando AWS Glue y S3 para procesar datos de ventas, inventario y clientes a trav√©s de la consola web de AWS.

**¬øQu√© hace el ETL?**
- üì• **Extract**: Lee archivos CSV desde S3 (facturas, inventario, usuarios)
- üîÑ **Transform**: Procesa y analiza los datos (ventas por cliente, productos m√°s vendidos, etc.)
- üì§ **Load**: Guarda los resultados procesados en S3

## üèóÔ∏è Arquitectura

```
S3 Bronze (datos raw)  ‚Üí  AWS Glue Job  ‚Üí  S3 Silver (datos procesados)
     ‚Üì                        ‚Üì                    ‚Üì
  tickets.csv            Transformaciones      ventas_por_cliente.csv
  items.csv                   ETL             productos_vendidos.csv
  clients.csv                                 ventas_por_dia.csv
```

## üöÄ Gu√≠a de Implementaci√≥n (Consola AWS)

### Paso 1: Crear Bucket S3

1. **Ir a la consola de S3**
   - Buscar "S3" en la consola de AWS
   - Hacer clic en "Create bucket"

2. **Configurar el bucket**
   - **Bucket name**: `datalake-utn`
   - **Region**: Seleccionar tu regi√≥n preferida (ej: us-east-1)
   - Dejar las dem√°s opciones por defecto
   - Hacer clic en "Create bucket"

3. **Crear carpetas**
   - Entrar al bucket `datalake-utn`
   - Hacer clic en "Create folder"
   - Crear carpeta llamada `bronze`
   - Hacer clic en "Create folder" nuevamente
   - Crear carpeta llamada `silver`

### Paso 2: Crear Rol IAM para Glue

1. **Ir a la consola de IAM**
   - Buscar "IAM" en la consola de AWS
   - En el men√∫ izquierdo, hacer clic en "Roles"

2. **Crear nuevo rol**
   - Hacer clic en "Create role"
   - **Trusted entity type**: AWS service
   - **Service or use case**: Glue
   - Hacer clic en "Next"

3. **Adjuntar pol√≠ticas**
   - Buscar y seleccionar: `AWSGlueServiceRole`
   - Buscar y seleccionar: `AmazonS3FullAccess`
   - Hacer clic en "Next"

4. **Configurar rol**
   - **Role name**: `GlueETLRole`
   - **Description**: `Rol para jobs de Glue ETL`
   - Hacer clic en "Create role"

### Paso 3: Preparar Archivos CSV

#### Crear archivos de ejemplo en tu computadora:

**tickets.csv**
```csv
ID_Cliente,Fecha,ID_Articulo,Cantidad,Total
1,2024-01-15,101,2,150.00
2,2024-01-16,102,1,75.50
3,2024-01-17,101,3,225.00
1,2024-01-18,103,1,89.99
2,2024-01-19,102,2,151.00
```

**items.csv**
```csv
ID_Articulo,Nombre_Articulo,Stock,Precio,Fecha_Ultima_Reposicion
101,Laptop HP,10,750.00,2024-01-10
102,Mouse Inal√°mbrico,25,75.50,2024-01-12
103,Teclado Mec√°nico,15,89.99,2024-01-14
```

**clients.csv**
```csv
ID_Cliente,Nombre,Apellido,Tipo_Cliente,Fecha_Registro
1,Juan,P√©rez,Premium,2023-06-15
2,Mar√≠a,Gonz√°lez,Regular,2023-08-20
3,Carlos,L√≥pez,Premium,2023-09-10
```

### Paso 4: Subir Archivos a S3

1. **Ir al bucket S3**
   - Volver a la consola de S3
   - Hacer clic en el bucket `datalake-utn`
   - Entrar a la carpeta `bronze`

2. **Subir archivos**
   - Hacer clic en "Upload"
   - Arrastar los 3 archivos CSV o hacer clic en "Add files"
   - Seleccionar: `tickets.csv`, `items.csv`, `clients.csv`
   - Hacer clic en "Upload"

3. **Verificar**
   - Confirmar que los 3 archivos aparezcan en `s3://datalake-utn/bronze/`

### Paso 5: Subir Script del Job Glue

Script Python Shell

2. **Subir script a S3**
   - Ir a S3 y buscar el bucket que empiece con `aws-glue-assets-`
   - Si no existe, crear bucket llamado `aws-glue-scripts-tu-nombre`
   - Crear carpeta `scripts` dentro del bucket
   - Subir el archivo `etl-script.py` a esa carpeta

### Paso 6: Crear Job de Glue

1. **Ir a la consola de AWS Glue**
   - Buscar "Glue" en la consola de AWS
   - En el men√∫ izquierdo, hacer clic en "Jobs"

2. **Crear nuevo job**
   - Hacer clic en "Create job"
   - **Job name**: `etl-pipeline-utn`
   - **IAM Role**: Seleccionar `GlueETLRole`
   - **Type**: Python shell script
   - **Glue version**: 3.0
   - **Python version**: 3.9

3. **Configurar script**
   - **Script path**: Buscar el archivo `etl-script.py` que subiste a S3
   - **Allocated capacity**: 0.0625 DPU
   - Hacer clic en "Create job"

### Paso 7: Ejecutar el Job

1. **En la p√°gina del job**
   - Hacer clic en "Run job"
   - Esperar a que el estado cambie a "Running"
   - El job deber√≠a completarse en 1-2 minutos

2. **Monitorear ejecuci√≥n**
   - En la pesta√±a "Runs", ver el progreso
   - Si el estado es "Succeeded", ¬°todo sali√≥ bien!
   - Si es "Failed", hacer clic en "Logs" para ver el error

### Paso 8: Verificar Resultados

1. **Ir a S3**
   - Navegar a `s3://datalake-utn/silver/`
   - Deber√≠as ver 3 archivos nuevos:
     - `ventas_por_cliente.csv`
     - `productos_mas_vendidos.csv`
     - `ventas_por_dia.csv`

2. **Descargar y revisar**
   - Hacer clic en cada archivo
   - Hacer clic en "Download" para verlos en Excel/Google Sheets

## üìä Resultados Esperados

**ventas_por_cliente.csv**
```csv
ID_Cliente,Total,Nombre,Apellido
1,239.99,Juan,P√©rez
2,226.50,Mar√≠a,Gonz√°lez
3,225.00,Carlos,L√≥pez
```

**productos_mas_vendidos.csv**
```csv
ID_Articulo,Cantidad,Nombre_Articulo
101,5,Laptop HP
102,3,Mouse Inal√°mbrico
103,1,Teclado Mec√°nico
```

**ventas_por_dia.csv**
```csv
Fecha,Total
2024-01-15,150.00
2024-01-16,75.50
2024-01-17,225.00
2024-01-18,89.99
2024-01-19,151.00
```

## üîß Monitoreo y Debugging

### Ver logs del job
1. **En AWS Glue**
   - Ir a "Jobs" ‚Üí Seleccionar tu job
   - Hacer clic en la pesta√±a "Runs"
   - Hacer clic en "Logs" de la ejecuci√≥n

2. **En CloudWatch**
   - Buscar "CloudWatch" en AWS
   - Ir a "Log groups"
   - Buscar `/aws-glue/python-jobs/etl-pipeline-utn`

### Soluci√≥n de problemas comunes

#### ‚ùå "Access Denied"
**Causa**: Permisos insuficientes
**Soluci√≥n**: 
- Verificar que el rol `GlueETLRole` tenga las pol√≠ticas correctas
- Ir a IAM ‚Üí Roles ‚Üí GlueETLRole ‚Üí Verificar pol√≠ticas adjuntas

#### ‚ùå "No such file or directory" 
**Causa**: Archivos no encontrados
**Soluci√≥n**:
- Verificar que los archivos CSV est√©n en `s3://datalake-utn/bronze/`
- Verificar nombres exactos: `tickets.csv`, `items.csv`, `clients.csv`

#### ‚ùå Job falla en transformaci√≥n
**Causa**: Estructura de datos incorrecta
**Soluci√≥n**:
- Verificar que los CSV tengan las columnas correctas
- Revisar logs para ver el error espec√≠fico

## üéØ Pr√≥ximos Pasos

1. **Automatizaci√≥n**
   - Crear trigger para ejecutar el job autom√°ticamente
   - Configurar notificaciones por email

2. **Expansi√≥n**
   - Agregar m√°s fuentes de datos
   - Crear visualizaciones con QuickSight

3. **Monitoreo**
   - Configurar alarmas en CloudWatch
   - Crear dashboard de m√©tricas

## üí° Tips

- **Costo**: Un job peque√±o cuesta ~$0.01-0.05 por ejecuci√≥n
- **Tiempo**: Jobs simples tardan 1-3 minutos
- **Testing**: Siempre probar con datos peque√±os primero
- **Backup**: Mantener copias de los scripts importantes

---

**¬°Tu pipeline ETL est√° funcionando!** üéâ

Para cualquier duda, revisar los logs en CloudWatch o contactar al equipo de soporte.